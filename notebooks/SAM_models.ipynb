{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "# pip install torchsummary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        self.img_size = 512 # org size mean 575 x 766\n",
    "        self.batch_size = 6 #6\n",
    "        self.seed = 42\n",
    "        self.workers = 3 #3\n",
    "        self.lr = 0.001\n",
    "config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhC(torch.utils.data.Dataset):\n",
    "    def __init__(self,transform):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        self.data_path = \"/dtu/datasets1/02516/PH2_Dataset_images\"\n",
    "        self.image_paths = sorted(glob.glob(f\"{self.data_path}/*/*_Dermoscopic_Image/*.bmp\"))\n",
    "        self.label_paths = sorted(glob.glob(f\"{self.data_path}/*/*_lesion/*_lesion.bmp\"))\n",
    "        # self.train_idx,val_test_idx = train_test_split(list(range(len(self.image_paths))),train_size=0.6,random_state=config.seed)\n",
    "        # self.val_idx, self.test_idx = train_test_split(val_test_idx,train_size=0.5,random_state=config.seed)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        # if idx in self.train_idx:\n",
    "        #     label = 'train'\n",
    "        # elif idx in self.val_idx:\n",
    "        #     label = 'val'\n",
    "        # elif idx in self.test_idx:\n",
    "        #     label = 'test'\n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        Y = self.transform(label)\n",
    "        X = self.transform(image)\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "\n",
    "size = config.img_size\n",
    "train_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "trainset = PhC(transform=train_transform)\n",
    "testset, val_test_set = torch.utils.data.random_split(trainset, [120, 80],generator=torch.Generator().manual_seed(config.seed))\n",
    "valset = PhC(transform=test_transform)\n",
    "testset = PhC(transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))\n",
    "val_loader = DataLoader(valset, batch_size=config.batch_size, shuffle=False, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))\n",
    "test_loader = DataLoader(testset, batch_size=config.batch_size, shuffle=False, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRIVE(torch.utils.data.Dataset):\n",
    "    def __init__(self,transform,idx_list):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        self.data_path = \"/dtu/datasets1/02516/DRIVE/training\"\n",
    "        self.image_paths = np.array(sorted(glob.glob(f\"{self.data_path}/images/*.tif\")))[idx_list]\n",
    "        self.label_paths = np.array(sorted(glob.glob(f\"{self.data_path}/1st_manual/*.gif\")))[idx_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        Y = self.transform(label)\n",
    "        X = self.transform(image)\n",
    "        return X, Y\n",
    "    \n",
    "\n",
    "size = config.img_size\n",
    "train_transform = transforms.Compose([transforms.Resize((size, size)),\n",
    "                                      transforms.RandomRotation(degrees=(10, 100)), \n",
    "                                    transforms.ToTensor()])\n",
    "val_test_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"/dtu/datasets1/02516/DRIVE/training\"\n",
    "image_paths = sorted(glob.glob(f\"{data_path}/images/*.tif\"))\n",
    "train_idx,val_test_idx = train_test_split(list(range(len(image_paths))),train_size=0.6,random_state=42)\n",
    "val_idx,test_idx = train_test_split(val_test_idx,train_size=0.5,random_state=42)\n",
    "\n",
    "trainset = DRIVE(transform=train_transform,idx_list=train_idx)\n",
    "valset = DRIVE(transform=val_test_transform,idx_list=val_idx)\n",
    "testset = DRIVE(transform=val_test_transform,idx_list=test_idx)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))\n",
    "val_loader = DataLoader(valset, batch_size=config.batch_size, shuffle=False, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))\n",
    "test_loader = DataLoader(testset, batch_size=config.batch_size, shuffle=False, num_workers=config.workers,generator=torch.Generator().manual_seed(config.seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(iter(train_loader))\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "val_images, val_labels = next(iter(val_loader))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(val_images[i], 0, 2), 0, 1))\n",
    "\n",
    "    plt.subplot(2, 4, i+5)\n",
    "    plt.imshow(val_labels[i].squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class metrics:\n",
    "    def __init__(self,eps:float=1e-8):\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_confusion(self,y_hat,mask):\n",
    "        # assuming y_hat is logits, then convert to confidences using sigmoid\n",
    "        if y_hat.min().item() < 0.0 or (y_hat.max().item() > 1.0):\n",
    "            y_hat = torch.sigmoid(y_hat)\n",
    "        y_hat = (y_hat > 0.50).float()\n",
    "\n",
    "        self.TP = (y_hat.flatten() * mask.flatten()).sum()\n",
    "        self.FN = mask[y_hat == 0].sum()\n",
    "        self.FP = y_hat[mask == 0].sum()\n",
    "        self.TN = y_hat.numel() - self.TP - self.FN - self.FP\n",
    "\n",
    "    def get_metrics(self,y_hat,mask):\n",
    "        self.get_confusion(y_hat,mask)\n",
    "        dice = ((2 * self.TP) / (2 * self.TP + self.FN + self.FP + self.eps)).item()\n",
    "        iou = ((self.TP) / (self.TP + self.FN + self.FP )).item()\n",
    "        acc = (self.TP+self.TN)/(self.TP+self.TN+self.FP+self.FN)\n",
    "        sensitivity = self.TP/(self.TP+self.FN)\n",
    "        specificity = self.TN/(self.TN+self.FP)\n",
    "        self.metric_dict = {'dice':dice,'iou':iou,'acc':acc,'sensitivity':sensitivity,'specificity':specificity}\n",
    "        return self.metric_dict \n",
    "    \n",
    "    def print_my_metrics(self,y_hat,mask,type_):\n",
    "        metric_dict = self.get_metrics(y_hat,mask)\n",
    "        for key in metric_dict:\n",
    "            print(f\"{type_} {key}: {metric_dict[key]}\")\n",
    "    \n",
    "evaluator = metrics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM Automatic masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"/zhome/21/8/156030/02516/deep-learning-in-computer-vison/models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "# device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im, lab = next(iter(test_loader_DRIVE))\n",
    "\n",
    "for no, (im_batch, lab_batch) in enumerate(test_loader):\n",
    "\n",
    "    for batch_no in range(len(im_batch)):\n",
    "        im = np.swapaxes(np.swapaxes(im_batch[batch_no].numpy(force=True), 0, 2), 0, 1)\n",
    "        im = (255 * im).astype(np.uint8)\n",
    "        # print(im0.shape)\n",
    "        # print(im0.dtype)\n",
    "        # plt.imshow(im0)\n",
    "        \n",
    "        lab = lab_batch[batch_no].squeeze().numpy(force=True)\n",
    "        # plt.imshow(lab0)\n",
    "        # print(lab0.shape)\n",
    "        # print(lab0.dtype)\n",
    "        # print(np.max(lab))\n",
    "\n",
    "        masks = mask_generator.generate(im)\n",
    "        # print(len(masks))\n",
    "        sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "\n",
    "        fig, axs = plt.subplots(1,2)\n",
    "        axs[0].imshow(im)\n",
    "        axs[1].imshow(lab)\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(1, min(5, len(sorted_masks)))\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            ax.imshow(sorted_masks[i][\"segmentation\"])\n",
    "        plt.show()\n",
    "\n",
    "        for i in range(min(5, len(sorted_masks))):\n",
    "            print(f\"Mask {i+1}:\")\n",
    "            evaluator.print_my_metrics(torch.from_numpy(sorted_masks[i][\"segmentation\"]), lab, '')\n",
    "            print(\"\\n\")\n",
    "\n",
    "        if batch_no >= 5:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# device = \"cuda\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sam \u001b[38;5;241m=\u001b[39m sam_model_registry[model_type](checkpoint\u001b[38;5;241m=\u001b[39msam_checkpoint)\n\u001b[0;32m----> 6\u001b[0m \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m predictor \u001b[38;5;241m=\u001b[39m SamPredictor(sam)\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/02516/DL_02516/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()\n",
    "sam_checkpoint = \"/zhome/21/8/156030/02516/deep-learning-in-computer-vison/models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "# device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "resize_transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
    "\n",
    "def prepare_image(image, transform, device):\n",
    "    image = transform.apply_image(image)\n",
    "    image = torch.as_tensor(image, device=device.device) \n",
    "    return image.permute(2, 0, 1).contiguous()\n",
    "\n",
    "# def prepare_input(image, points, labels, boxes, sam):\n",
    "#     rezise_transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
    "#     batched_input = [\n",
    "#         {\n",
    "#          'image': prepare_image(image, resize_transform, sam),\n",
    "#          'points_coords': resize_transform.apply_coords_torch(points, image.shape[:2]),\n",
    "#          'poinnt_labels': labels,\n",
    "#          'boxes': resize_transform.apply_boxes_torch(boxes, image.shape[:2]),\n",
    "#          'original_size': image.shape[:2]\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "#     return batched_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no, (im_batch, lab_batch) in enumerate(test_loader):\n",
    "\n",
    "    for batch_no in range(len(im_batch)):\n",
    "        im = np.swapaxes(np.swapaxes(im_batch[batch_no].numpy(force=True), 0, 2), 0, 1)\n",
    "        im = (255 * im).astype(np.uint8)\n",
    "\n",
    "        \n",
    "        lab = lab_batch[batch_no].squeeze().detach().cpu().numpy(force=True)\n",
    "\n",
    "        xs, ys = np.where(lab > 0.9)\n",
    "        points = np.array([[y, x] for x,y in zip(xs, ys)])\n",
    "        # print(len(points))\n",
    "        nxs, nys = np.where(lab <= 0.5)\n",
    "        n_points = np.array([[y, x] for x,y in zip(nxs, nys)])\n",
    "\n",
    "        points = np.mean(points, axis=0).reshape(-1,2)\n",
    "        n_points = []\n",
    "        # points = points[np.random.choice(np.arange(len(points)), size=3, replace=False)].reshape(-1,2)\n",
    "        # n_points = n_points[np.random.choice(np.arange(len(n_points)), size=1, replace=False)]\n",
    "        # input_points = np.concatenate((points, n_points))\n",
    "        input_points = points\n",
    "        # input_points = points[13000:16000]\n",
    "        # input_labels = np.array([1, 1, 1])\n",
    "        input_labels = np.array([1]*input_points.shape[0])\n",
    "        # input_labels = np.array([1]*len(points) + [0]*len(n_points))\n",
    "        # input_labels = np.array([1])\n",
    "\n",
    "        print(input_points.shape)\n",
    "        print(input_labels.shape)\n",
    "\n",
    "        input_box = np.array([400, 200, 460, 260])\n",
    "\n",
    "        batch_input = [\n",
    "            {\n",
    "                'image': prepare_image(im, resize_transform, sam),\n",
    "                'points_coords': resize_transform.apply_coords_torch(torch.from_numpy(input_points), im.shape[:2]),\n",
    "                'point_labels': input_labels,\n",
    "                # 'boxes': resize_transform.apply_boxes_torch(torch.from_numpy(input_box), im.shape[:2]),\n",
    "                'original_size': im.shape[:2]\n",
    "            }\n",
    "        ]\n",
    "        batched_output = sam(batch_input, multimask_output=True)\n",
    "\n",
    "        # print(batched_output[0]['masks'].size())\n",
    "        masks = batched_output[0]['masks'][0]\n",
    "        # print(masks.shape)\n",
    "        # print(masks[0].shape)\n",
    "\n",
    "        # predictor.set_image(im)\n",
    "        # masks, scores, logits = predictor.predict(\n",
    "        #     point_coords=input_points,\n",
    "        #     point_labels=input_labels,\n",
    "        #     box = input_box[None, :],\n",
    "        #     multimask_output=True\n",
    "        # )\n",
    "\n",
    "        # print(masks.shape)\n",
    "        # fig, axs = plt.subplots(1,3)\n",
    "        # axs[0].imshow(im)\n",
    "        # axs[1].imshow(lab)\n",
    "        # # axs[2].imshow(masks[0,:,:])\n",
    "        # axs[2].imshow(im)\n",
    "        # show_mask(masks, axs[2])\n",
    "        # show_points(input_points, input_labels, axs[2])\n",
    "        # show_box(input_box, axs[2])\n",
    "\n",
    "        # plt.show()\n",
    "        fig, axs = plt.subplots(1,2)\n",
    "        axs[0].imshow(im)\n",
    "        axs[1].imshow(lab)\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(1, min(5, masks.shape[0]))\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            # ax.imshow(masks[i])\n",
    "            ax.imshow(im)\n",
    "            show_mask(masks[i].cpu().numpy(), ax)\n",
    "            show_points(input_points, input_labels, ax)\n",
    "            # show_box(input_box, ax)\n",
    "        plt.show()\n",
    "\n",
    "        for i, mask in enumerate(masks):\n",
    "            print(f\"Mask {i+1}:\")\n",
    "            evaluator.print_my_metrics(masks[i].cpu(), lab, '')\n",
    "            print(\"\\n\")\n",
    "        # masks = mask_generator.generate(im)\n",
    "        # # print(len(masks))\n",
    "        # sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "\n",
    "        # fig, axs = plt.subplots(1,2)\n",
    "        # axs[0].imshow(im)\n",
    "        # axs[1].imshow(lab)\n",
    "        # plt.show()\n",
    "\n",
    "        # fig, axs = plt.subplots(1, min(5, len(sorted_masks)))\n",
    "        # for i, ax in enumerate(axs.flat):\n",
    "        #     ax.imshow(sorted_masks[i][\"segmentation\"])\n",
    "        # plt.show()\n",
    "\n",
    "        break\n",
    "        \n",
    "\n",
    "        \n",
    "    if no >= 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
