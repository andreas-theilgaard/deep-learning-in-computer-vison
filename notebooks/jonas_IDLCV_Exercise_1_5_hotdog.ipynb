{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRIhx7PugJy3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import pandas as pd\n",
    "# import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35PhqXpWUZ7I"
   },
   "source": [
    "We always check that we are running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_gOv_pUZeB"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAj64PJYgJzC"
   },
   "source": [
    "We provide you with a class that can load the *hotdog/not hotdog* dataset you should use from /dtu/datasets1/02516/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mUlnOuzgJzF"
   },
   "outputs": [],
   "source": [
    "class Hotdog_NotHotdog(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path='/zhome/21/8/156030/02516/hotdog_nothotdog'):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path +'/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JewkmhKlgJzN"
   },
   "source": [
    "Below is the simple way of converting the images to something that can be fed through a network.\n",
    "Feel free to use something other than $128\\times128$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcilkL3dgJzP"
   },
   "outputs": [],
   "source": [
    "# Rotating, scaling, flipping, noise, color, crop\n",
    "size = 128\n",
    "# crop_size = (80, 80)\n",
    "degree = 20\n",
    "color_jit = [0.2, 0.15, 0.1, 0.15]\n",
    "affine = [20, (0.2,0.3), None, 30]\n",
    "gaus_kernel = (5, 5)\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(degrees=degree),\n",
    "                                    # transforms.RandomAffine(*affine),\n",
    "                                    # transforms.RandomCrop(crop_size),\n",
    "                                    transforms.GaussianBlur(gaus_kernel, sigma=(0.01, 2.0)),\n",
    "                                    transforms.RandomHorizontalFlip(p=0.3),\n",
    "                                    transforms.RandomVerticalFlip(p=0.3),\n",
    "                                    transforms.ColorJitter(*color_jit),  \n",
    "                                    transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomRotation(degrees=degree),\n",
    "                                    # transforms.RandomAffine(*affine),\n",
    "                                    # transforms.RandomCrop(crop_size),\n",
    "                                    transforms.GaussianBlur(gaus_kernel, sigma=(0.01, 2.0)),\n",
    "                                    transforms.RandomHorizontalFlip(p=0.3),\n",
    "                                    transforms.RandomVerticalFlip(p=0.3),\n",
    "                                    transforms.ColorJitter(*color_jit),  \n",
    "                                    transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "# size = 128\n",
    "# train_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "#                                     transforms.ToTensor()])\n",
    "# test_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "#                                     transforms.ToTensor()])\n",
    "batch_size = 64\n",
    "trainset = Hotdog_NotHotdog(train=True, transform=train_transform)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "testset = Hotdog_NotHotdog(train=False, transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho-YRb6HgJzZ"
   },
   "source": [
    "Let's look at some images from our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm4Ara7dgJza"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(int(len(images)/7) + 1,7,i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(images[i].numpy(), 0, 2), 0, 1))\n",
    "    plt.title(['hotdog', 'not hotdog'][labels[i].item()])\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgcJoEkzQ5da"
   },
   "source": [
    "Remember to save the state of your model AND optimizer regularly in case the Colab runtime times out.\n",
    "You can save your model to your google drive, so you can get it from there in a new colab session. \n",
    "\n",
    "If you only save it in the colab notebook, there's no way to get it into a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FKkPUPmQ_r0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12N0EYYsQPhJ"
   },
   "source": [
    "Now create a model and train it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 8, 3, 1, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(8, 8, 3, 1, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(8, 16, 3, 1, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, 3, 1, padding=1),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "                nn.Linear(65536, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 1),\n",
    "                nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer_SGD = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer_Adam = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    def loss_fun(output, target):\n",
    "        return criterion(output, target)\n",
    "        # return F.nll_loss(torch.log(output), target)\n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            target = target.float()\n",
    "            #Compute the loss\n",
    "            # print(output.shape)\n",
    "            # print(target.shape)\n",
    "            loss = loss_fun(output, target)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.round() == target\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            target = target.float()\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            test_loss.append(loss_fun(output, target).cpu().item())\n",
    "            predicted = output.round() == target\n",
    "            test_correct += (target==predicted).sum().cpu().item()\n",
    "        out_dict['train_acc'].append(train_correct/len(trainset))\n",
    "        out_dict['test_acc'].append(test_correct/len(testset))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['test_loss'].append(np.mean(test_loss))\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "out_dict_SGD = train(model, optimizer_SGD, num_epochs=num_epochs)\n",
    "out_dict_adam = train(model, optimizer_Adam, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "epochs = np.arange(num_epochs)\n",
    "fig.suptitle('Training results')\n",
    "\n",
    "axs[0, 0].plot(epochs, out_dict_SGD['train_acc'], label=\"Train\")\n",
    "axs[0, 0].plot(epochs, out_dict_SGD['test_acc'], label=\"Test\")\n",
    "axs[0, 0].set_title(\"SGD\")\n",
    "axs[0, 0].set(xlabel=\"Epochs\", ylabel='Accuracy')\n",
    "\n",
    "axs[1, 0].plot(epochs, out_dict_SGD['train_loss'], label=\"Train\")\n",
    "axs[1, 0].plot(epochs, out_dict_SGD['test_loss'], label=\"Test\")\n",
    "axs[1, 0].set(xlabel=\"Epochs\", ylabel='Loss')\n",
    "\n",
    "axs[0, 1].plot(epochs, out_dict_adam['train_acc'], label=\"Train\")\n",
    "axs[0, 1].plot(epochs, out_dict_adam['test_acc'], label=\"Test\")\n",
    "axs[0, 1].set_title(\"Adam\")\n",
    "axs[0, 1].set(xlabel=\"Epochs\", ylabel='Accuracy')\n",
    "\n",
    "axs[1, 1].plot(epochs, out_dict_adam['train_loss'], label=\"Train\")\n",
    "axs[1, 1].plot(epochs, out_dict_adam['test_loss'], label=\"Test\")\n",
    "axs[1, 1].set(xlabel=\"Epochs\", ylabel='Loss')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(0, num_epochs)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.label_outer()\n",
    "fig.legend([\"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "CM = np.array([[0,0],[0,0]])\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    target = target.float()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "    # test_loss.append(loss_fun(output, target).cpu().item())\n",
    "    predicted = output.round() == target\n",
    "    for p, t in zip(predicted, target):\n",
    "        p = int(p.cpu().item())\n",
    "        t = int(t.cpu().item())\n",
    "        CM[p, t] += 1\n",
    "    # test_correct += (target==predicted).sum().cpu().item()\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(CM, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "data, target = images.to(device), labels.to(device)\n",
    "\n",
    "# target = target.float()\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "# test_loss.append(loss_fun(output, target).cpu().item())\n",
    "predicted = output.round() == target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "data, target = images.to(device), labels.to(device)\n",
    "\n",
    "# target = target.float()\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "# test_loss.append(loss_fun(output, target).cpu().item())\n",
    "predicted = output.round() == target\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(int(len(images)/7) + 1,7,i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(images[i].numpy(), 0, 2), 0, 1))\n",
    "    if predicted[i].item() == labels[i].item():\n",
    "        plt.title(['hotdog', 'not hotdog'][predicted[i].item()], dict(color=\"green\"))\n",
    "    else:\n",
    "        plt.title(['hotdog', 'not hotdog'][predicted[i].item()], dict(color=\"red\"))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = data.to(device), target.to(device)\n",
    "target = target.float()\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "test_loss.append(loss_fun(output, target).cpu().item())\n",
    "predicted = output.round() == target\n",
    "test_correct += (target==predicted).sum().cpu().item()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project 1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
